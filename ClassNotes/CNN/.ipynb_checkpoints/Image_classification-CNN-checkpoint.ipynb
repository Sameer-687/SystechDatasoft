{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "382d1836",
   "metadata": {},
   "source": [
    "# CNN:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/conv.gif\" alt=\"python CNN\" width=94% height=85% title=\"CNN Convolution\"> \n",
    "\n",
    "A Convolutional Neural Network (CNN) is a type of deep learning model primarily used for image processing and analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e108a",
   "metadata": {},
   "source": [
    "# Deep Learning on large Images\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/deep_large_img.jpg\" alt=\"large img\" width=94% height=85%> \n",
    "\n",
    "If we work with small 64 by 64 images, which have 3 color channels (red, green, and blue). This made for a total of 12,288 input features, which was manageable. But when dealing with larger images, like 1,000 by 1,000 pixels (1 megapixel), the input features become huge. For instance, a 1,000 by 1,000 image with 3 RGB channels results in 3 million input features.\n",
    "\n",
    "Now, let's talk about neural networks. In a neural network, the first hidden layer often has some number of hidden units. Let's say there are 1,000 hidden units in this layer.\n",
    "\n",
    "In the earlier large image example, this would mean the weight matrix (W1) connecting the input features to the hidden units would have dimensions of 1,000 by 3 million. This is a massive matrix with 3 billion parameters (values), which is really hard to manage because it requires a vast amount of data to train effectively, and it's computationally intensive.\n",
    "\n",
    "To make things easier, especially for computer vision tasks, we need to use convolutional neural networks (CNNs). These networks are designed to work well with large images and reduce the number of parameters by using a clever technique called convolution, making it more practical to work with high-resolution images like 1,000 by 1,000 or even larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f32ec7",
   "metadata": {},
   "source": [
    "## What a CNN can see\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/layer_detect.jpg\" alt=\"large img\" width=94% height=85%> \n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/vertical_edge.gif\" alt=\"large img\" width=94% height=85%> \n",
    "\n",
    "In this section, we'll explore how convolution works, a crucial component of Convolutional Neural Networks (CNNs). In CNNs, early layers often detect edges in images. For instance, if you want to identify objects in a picture, you might start by detecting vertical and horizontal edges.\n",
    "\n",
    "Let's consider an example: a grayscale 6x6 image. To find vertical edges, we use a 3x3 filter. The convolution operation combines the image and filter. We slide the filter across the image, element-wise multiplying and summing to get a 4x4 output.\n",
    "\n",
    "To compute each element of the output, we place the filter on a 3x3 region of the input and perform element-wise multiplication and addition. This process repeats across the image, giving us the 4x4 output matrix.\n",
    "\n",
    "In this example, we used a filter designed to detect vertical edges. It highlights areas in the image where there's a transition from light to dark or vice versa. This output acts as a \"vertical edge detector.\"\n",
    "\n",
    "In practice, CNNs use convolution as a fundamental operation to detect various features in images, starting with edges and progressing to more complex patterns.\n",
    "\n",
    "### Vertical Edge Detector:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/vertical_edge.png\" alt=\"large img\" width=94% height=85%> \n",
    "\n",
    "Imagine a 6x6 picture split in half. The left side is bright with \"10\" values, and the right side is dark with \"0\" values. When we use a special 3x3 filter on this image, it looks for changes from bright to dark.\n",
    "\n",
    "This filter detects a strong vertical line down the middle where the image transitions from bright to dark. When we apply this filter, we get a matrix that shows this vertical line. It's like the filter is highlighting the edge.\n",
    "\n",
    "Keep in mind, in real applications with larger images, this edge detection works just as effectively, even if it might look thicker on a small image like our 6x6 example.\n",
    "\n",
    "In simple terms, this process helps a computer find vertical edges in pictures. It's a fundamental part of building Convolutional Neural Networks, which are used for recognizing objects in images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d04dede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a binary image with the left half as 10s and the right half as 0s with hardcoded values\n",
    "input_tensor = torch.tensor([[10, 10, 10, 0, 0, 0],\n",
    "                             [10, 10, 10, 0, 0, 0],\n",
    "                             [10, 10, 10, 0, 0, 0],\n",
    "                             [10, 10, 10, 0, 0, 0],\n",
    "                             [10, 10, 10, 0, 0, 0],\n",
    "                             [10, 10, 10, 0, 0, 0]], dtype=torch.float32).view(1, 1, 6, 6)  # (batch_size, channels, height, width)\n",
    "\n",
    "# Define a vertical edge detector kernel\n",
    "kernel = torch.tensor([[1, 0, -1],\n",
    "                       [1, 0, -1],\n",
    "                       [1, 0, -1]], dtype=torch.float32).view(1, 1, 3, 3)  # (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "# Perform convolution without padding\n",
    "edge_detection_result = F.conv2d(input_tensor, kernel)\n",
    "\n",
    "# Convert the tensors to NumPy arrays for visualization\n",
    "input_image = input_tensor[0, 0].detach().numpy()\n",
    "kernel_image = kernel[0, 0].detach().numpy()\n",
    "edge_detection_image = edge_detection_result[0, 0].detach().numpy()\n",
    "\n",
    "# Define a smaller figure size\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))  # Adjust the figsize here\n",
    "\n",
    "axs[0].imshow(input_image, cmap='gray', vmin=0, vmax=10)\n",
    "axs[0].set_title('Binary Input Image')\n",
    "\n",
    "axs[1].imshow(kernel_image, cmap='gray')\n",
    "axs[1].set_title('Vertical Edge Detector Kernel')\n",
    "\n",
    "axs[2].imshow(edge_detection_image, cmap='gray', vmin=-10, vmax=10)\n",
    "axs[2].set_title('Edge Detection Result')\n",
    "\n",
    "# Add spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81164fc6",
   "metadata": {},
   "source": [
    "### More detector example\n",
    "\n",
    "#### Sobel:\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/sobel.png\" alt=\"large img\" width=60% height=60%> \n",
    "\n",
    "#### Comparison\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/canny.jpg\" alt=\"large img\" width=60% height=60%> \n",
    "\n",
    "\n",
    "### After applying on Image\n",
    "\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/edge_detector.png\" alt=\"large img\" width=80% height=80%> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6036b",
   "metadata": {},
   "source": [
    "### How cnn solves the problem:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/weight_filter.jpg\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "With the emergence of deep learning, we've discovered that when we want to identify edges in a complex image, we might not need computer vision experts to manually select nine specific values. Instead, we can allow the neural network to learn these nine values as parameters through backpropagation. The objective is to optimize these nine parameters so that when we apply them as a 3x3 filter to a 6x6 image, it effectively detects edges in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5b2fa",
   "metadata": {},
   "source": [
    "### Padding in Convolutional Operations\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/zero_padding.gif\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "\n",
    "\n",
    "In order to build deep neural networks, it's important to understand the concept of **padding** when performing convolutional operations. Padding involves adding extra pixels around the edges of an image before convolution. This is crucial for two reasons:\n",
    "\n",
    "1. **Preserving Image Size:** Without padding, convolution operations can cause the image to shrink, which can be problematic in deep networks where repeated convolutions can make the image very small. Padding allows you to maintain the original image size.\n",
    "\n",
    "2. **Information Preservation:** Padding ensures that pixels near the edges of the image are more effectively utilized in the convolution process, preventing the loss of valuable information from the image edges.\n",
    "\n",
    "The amount of padding (denoted as \"p\") can be determined based on whether you want \"Valid\" or \"Same\" convolutions:\n",
    "\n",
    "- \"Valid\" convolution means no padding, resulting in a smaller output size.\n",
    "- \"Same\" convolution means padding the image so that the output size matches the input size. For an odd-sized filter \"f,\" you can calculate the required padding as (f - 1) / 2.\n",
    "\n",
    "In computer vision, odd-sized filters (e.g., 3x3, 5x5) are commonly used due to their symmetric nature and the presence of a central pixel. This convention simplifies padding choices.\n",
    "\n",
    "Understanding padding is essential when implementing convolutional neural networks (CNNs), and it helps ensure that important image information is retained throughout the network's layers. In the next video, the discussion will shift to implementing Strided convolutions.\n",
    "\n",
    "The formula for calculating the dimension of the output feature map after a convolution operation is:\n",
    "\n",
    "$ \\text{New Dimension} = n - f + 1 $\n",
    "\n",
    "Where:\n",
    "- $ \\text{New Dimension} $ is the size of the output feature map.\n",
    "- $ n $ represents the size (width or height) of the input image.\n",
    "- $ f $ represents the size (width or height) of the convolutional filter used.\n",
    "\n",
    "### Valid Convolution (No Padding):\n",
    "- Input Image Size ($n$): 6x6\n",
    "- Filter Size ($f$): 3x3\n",
    "\n",
    "Using the formula $ \\text{New Dimension} = n - f + 1 $, we calculate the output size:\n",
    "\n",
    "For Valid Convolution:\n",
    "$ \\text{New Dimension} = 6 - 3 + 1 = 4 $\n",
    "\n",
    "So, for valid convolution, the output dimension is 4x4.\n",
    "\n",
    "### Same Convolution (With Padding):\n",
    "- Input Image Size ($n$): 6x6\n",
    "- Filter Size ($f$): 3x3\n",
    "\n",
    "Using the formula $ \\text{New Dimension} = n + 2p - f + 1 $, we calculate the required padding ($p$) for \"Same\" convolution to maintain the same input and output dimensions:\n",
    "\n",
    "- We want the output dimension to be the same as the input dimension, which is 6x6.\n",
    "\n",
    "$ \\text{New Dimension} = 6 + 2p - 3 + 1 = 6 \\\\ $ \n",
    "$ 2p - 2 = 0 \\\\ $\n",
    "$ 2p = 2 \\\\ $\n",
    "$ p = 1 \\\\ $\n",
    "\n",
    "So, for \"Same\" convolution, you would need to pad the input image with 1 pixel of padding on all sides to maintain the input and output dimensions at 6x6.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77621a",
   "metadata": {},
   "source": [
    "## Stride Convolutions in Convolutional Neural Networks\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/strided_conv1.gif\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "Understanding stride convolutions is essential when working with Convolutional Neural Networks (CNNs). In this context, let's explore an example to illustrate how stride convolutions work.\n",
    "\n",
    "Suppose you want to convolve a 7x7 image with a 3x3 filter, but with a stride of two. This means that instead of moving the filter one pixel at a time, you move it by two pixels. Here's the process:\n",
    "\n",
    "1. Take the element-wise product in the upper-left 3x3 region, sum it, and get $91$.\n",
    "2. Move the blue box over by two steps to the right, resulting in an element-wise product sum of $100$.\n",
    "3. Again, shift the box by two steps, yielding a sum of $83$.\n",
    "4. Move to the next row and skip over one position by taking two steps to the right, resulting in a sum of $69$.\n",
    "5. Move over by two steps again, giving a sum of $91$, and so on.\n",
    "6. This process results in a $3x3$ output.\n",
    "\n",
    "The dimensions of the input and output are determined by a formula:\n",
    "\n",
    "If you have an $n x n$ image, convolve it with an $f x f$ filter, use padding of $p$, and a stride of $s$, then the output size is given by:\n",
    "\n",
    "$ \\text{Output Dimension} = \\left\\lfloor \\frac{n + 2p - f}{s} \\right\\rfloor + 1 $\n",
    "\n",
    "In this example, with a $7x7$ image, $3x3$ filter, no padding ($p=0$), and a stride of $2$, you get:\n",
    "\n",
    "$ \\text{Output Dimension} = \\left\\lfloor \\frac{7 + 2(0) - 3}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{4}{2} \\right\\rfloor + 1 = 2 + 1 = 3 $\n",
    "\n",
    "So, you end up with a $3x3$ output.\n",
    "\n",
    "\n",
    "To summarize, stride convolutions are a crucial part of CNNs, allowing you to control the output size and stride while convolving over images or volumes. In the next chapter, we'll explore convolutions over volumes, further enhancing the capabilities of this operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d686ad",
   "metadata": {},
   "source": [
    "### Convolution in Three-Dimensional Volumes:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/conv_volume.gif\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "In the realm of convolutional neural networks (CNNs), we encounter scenarios where we need to apply convolution operations not just to 2D images but to three-dimensional volumes. Let's delve into an example to grasp this concept.\n",
    "\n",
    "Consider the desire to detect features not only in grayscale images but in RGB images. An RGB image is different from a grayscale one; it's a 3D volume. Instead of being a simple 2D matrix, it consists of three layers, each representing a color channel: red, green, and blue. So, a 6x6 RGB image is essentially three 6x6 images stacked together.\n",
    "\n",
    "To achieve feature detection in such an RGB image, we employ a 3D filter, typically sized as 3x3x3. This filter comprises three layers, mirroring the red, green, and blue channels of the image. Crucially, the number of channels in the image must match that of the filter.\n",
    "\n",
    "Now, let's demystify the convolution process. Imagine a simplified scenario where we position a 3x3x3 filter in the upper-left corner of the image. This filter holds 27 parameters, nine for each color channel. We compute the output by multiplying these parameters with the corresponding values in the image for each channel, summing them up, and obtaining the first output value.\n",
    "\n",
    "To proceed, we slide the filter one step to the right, conducting 27 multiplications and summations again, yielding the next output value. We repeat this operation throughout the image, shifting the filter horizontally and vertically, until we arrive at the final 4x4 output.\n",
    "\n",
    "By adjusting the filter parameters, we can create various feature detectors. For instance, setting the filter to have ones only in the red channel and zeros in the green and blue channels allows us to detect vertical edges in the red channel. If we aim to detect edges regardless of color, we can configure the filter to have alternating ones and minus ones across all channels.\n",
    "\n",
    "In the domain of computer vision, the convention is that the filter can have different heights and widths while maintaining the same number of channels as the input. This approach provides flexibility in designing filters that focus on specific channels or amalgamate information from all channels.\n",
    "\n",
    "To sum up, applying convolution to a three-dimensional volume, like a 6x6x3 RGB image, using a 3x3x3 filter, yields a 2D output, in this case, a 4x4 image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c316723",
   "metadata": {},
   "source": [
    "### Multiple filters: \n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/mul_filter.png\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "Certainly, the next crucial concept in building convolutional neural networks (CNNs) involves using multiple filters simultaneously to detect various features. In the previous example, we convolved a 6x6x3 input image with a 3x3x3 filter to obtain a 4x4 output. This filter could be thought of as a vertical edge detector.\n",
    "\n",
    "Now, imagine we want to detect not just vertical edges but also horizontal edges, diagonal edges, and perhaps other features simultaneously. We can achieve this by using multiple filters. Let's say we have a second filter, denoted in orange, which could be a horizontal edge detector. When we convolve the input image with this second filter, it results in a different 4x4 output.\n",
    "\n",
    "We can then stack these two outputs together, placing the first one in front and the second one behind, essentially creating a 4x4x2 output volume. This means that by applying two different 3x3 filters to our 6x6x3 image, we obtain two 4x4 outputs, which are then combined to form a $4 \\times 4 \\times 2$ volume.\n",
    "\n",
    "To summarize the dimensions:\n",
    "\n",
    "- If you have an $n \\times n$ input image with a number of channels (e.g., $n \\times n \\times 3$ for an RGB image).\n",
    "- And you convolve it with an $f \\times f$ filter with the same number of channels.\n",
    "- The result is an output volume of size $(n - f + 1) \\times (n - f + 1) \\times nC'$, where $nC'$ is the number of filters used.\n",
    "\n",
    "This concept of convolutions over volumes is powerful because it allows us to detect multiple features simultaneously, resulting in an output with a number of channels equal to the number of filters used. This sets the stage for building deeper CNNs with diverse feature detection capabilities. Note that the term \"channels\" is used to refer to the size of the third dimension in these filters, although \"depth\" is another term often used in the literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b70fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Layer Output Shape: torch.Size([1, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Demo input data: Batch size 1, 3 channels (RGB), 6x6 image\n",
    "demo_input = torch.randn(1, 3, 6, 6)\n",
    "\n",
    "# Define the Convolutional Layer\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, padding=0)\n",
    "\n",
    "# Pass the input through the convolutional layer\n",
    "output = conv_layer(demo_input)\n",
    "\n",
    "print(\"Convolutional Layer Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009870db",
   "metadata": {},
   "source": [
    "## One Layer of a Convolutional Network:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/cnn_one_layer.png\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "Let's explore how to construct a single layer of a Convolutional Neural Network (CNN) through an illustrative example. In the previous chapter, we learned how to convolve a 3D volume with two different filters, resulting in two distinct 4x4 outputs. To create a CNN layer, we'll introduce biases for each of these outputs and apply a non-linear activation function, such as ReLU, to obtain a 4x4 output after these transformations.\n",
    "\n",
    "Here's the step-by-step process:\n",
    "\n",
    "1. **Convolution with Filters:** We start with our 3D input volume, which can be thought of as a 6x6x3 image. We convolve this input with two different 3x3 filters, resulting in two separate 4x4 outputs. Each filter performs a weighted sum of the input values within its receptive field.\n",
    "\n",
    "2. **Bias Addition:** After the convolution, we add a bias term to each element of both 4x4 outputs. This bias is a real number and is added uniformly to all 16 elements in each output. Python's broadcasting mechanism simplifies this operation.\n",
    "\n",
    "3. **Non-Linear Activation:** Following the bias addition, we apply a non-linear activation function (e.g., ReLU) element-wise to each output. This introduces non-linearity into the layer.\n",
    "\n",
    "4. **Stacking Outputs:** Finally, we stack these two 4x4 outputs together, placing the first output in the front and the second one behind it. This stacking creates a 4x4x2 output volume.\n",
    "\n",
    "This transformation, which takes us from a 6x6x3 input to a 4x4x2 output, constitutes one layer of a convolutional neural network.\n",
    "\n",
    "Now, let's draw parallels between this CNN layer and a single layer in a traditional feedforward neural network (FNN):\n",
    "\n",
    "In an FNN layer, we typically have the following:\n",
    "\n",
    "```python\n",
    "z1 = w1 * a0 + b[1]\n",
    "a[1] = g(z[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce727f7",
   "metadata": {},
   "source": [
    "### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fbf0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Layer Output Shape after ReLU: torch.Size([1, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Import the functional module for activations\n",
    "\n",
    "# Demo input data: Batch size 1, 3 channels (RGB), 6x6 image\n",
    "demo_input = torch.randn(1, 3, 6, 6)\n",
    "\n",
    "# Define the Convolutional Layer\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, padding=0)\n",
    "\n",
    "# Pass the input through the convolutional layer\n",
    "output = conv_layer(demo_input)\n",
    "\n",
    "# Apply ReLU activation function to the output\n",
    "output = F.relu(output)\n",
    "\n",
    "print(\"Convolutional Layer Output Shape after ReLU:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795eba9",
   "metadata": {},
   "source": [
    "### Number of parameters in one layer\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/num_para.png\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "Let's suppose you have 10 filters, not just two filters, that are 3 by 3 by 3, and one layer of a neural network. How many parameters does this layer have? Well, let's figure this out.\n",
    "\n",
    "Each filter is a 3 x 3 x 3 volume, so it has 3 x 3 x 3 = 27 parameters, plus the bias term, which adds 1 more parameter. So, each filter has a total of 28 parameters.\n",
    "\n",
    "Now, imagine that you actually have ten of these filters. That means you have 10 sets of these 28 parameters. So, the total number of parameters for this layer would be 28 parameters per filter multiplied by 10 filters, resulting in a total of 280 parameters for this layer.\n",
    "\n",
    "One interesting aspect to note is that regardless of the size of the input image, whether it's 1,000 by 1,000 pixels or even larger, the number of parameters in this layer remains fixed at 280. This efficiency allows you to use these ten filters to detect various features, including vertical and horizontal edges, and other patterns, even in very large images, without significantly increasing the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee928536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Layer Output Shape after ReLU: torch.Size([1, 10, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Import the functional module for activations\n",
    "\n",
    "# Demo input data: Batch size 1, 3 channels (RGB), 6x6 image\n",
    "demo_input = torch.randn(1, 3, 6, 6)\n",
    "\n",
    "# Define the Convolutional Layer\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, padding=0)\n",
    "\n",
    "# Pass the input through the convolutional layer\n",
    "output = conv_layer(demo_input)\n",
    "\n",
    "# Apply ReLU activation function to the output\n",
    "output = F.relu(output)\n",
    "\n",
    "print(\"Convolutional Layer Output Shape after ReLU:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4935ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Convolutional Layer: 280\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of parameters in the convolutional layer\n",
    "total_params = sum(p.numel() for p in conv_layer.parameters())\n",
    "print(\"Total Parameters in Convolutional Layer:\", total_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a05df",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/notation.png\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "## Layer Notation\n",
    "- $l$: Layer index, indicating the specific layer in the CNN.\n",
    "- $f^{[l]}$: Filter size, denoting the dimensions of the filters used in layer $l$.\n",
    "- $p^{[l]}$: Padding, representing the amount of padding applied in layer $l$.\n",
    "- $s^{[l]}$: Stride, indicating the stride used in layer $l$.\n",
    "\n",
    "## Input and Output Dimensions\n",
    "- $nH^{[l-1]}, nW^{[l-1]}, nC^{[l-1]}$: Height, width, and number of channels of the input from the previous layer (layer $l-1$).\n",
    "- $nH^{[l]}, nW^{[l]}, nC^{[l]}$: Height, width, and number of channels of the output of layer $l$.\n",
    "\n",
    "## Output Size Formula\n",
    "- The height ($nH^{[l]}$) and width ($nW^{[l]}$) of the output volume are determined using a formula based on the input size, padding, filter size, and stride.\n",
    "- These dimensions are calculated separately for height and width.\n",
    "\n",
    "## Number of Channels (Filters)\n",
    "- The number of channels in the output volume ($nC^{[l]}$) is equal to the number of filters used in layer $l$.\n",
    "\n",
    "## Filter Dimensions\n",
    "- Each filter in layer $l$ has dimensions $f^{[l]} \\times f^{[l]} \\times nC^{[l-1]}$, where $nC^{[l-1]}$ is the number of channels in the input from the previous layer.\n",
    "\n",
    "## Weights (Parameters)\n",
    "- The weights, often represented by the filters, collectively form a tensor with dimensions $f^{[l]} \\times f^{[l]} \\times nC^{[l-1]} \\times nC^{[l]}$, where $nC^{[l]}$ is the number of filters used in layer $l$.\n",
    "\n",
    "## Bias Parameters\n",
    "- Each filter has an associated bias parameter, resulting in $nC^{[l]}$ bias parameters.\n",
    "\n",
    "## Activation Output\n",
    "- The activation output of layer $l$, denoted as $a^{[l]}$, is a 3D volume with dimensions $nH^{[l]} \\times nW^{[l]} \\times nC^{[l]}$.\n",
    "\n",
    "## Batch Processing\n",
    "- In batch gradient descent or mini-batch gradient descent, activations are represented as $A^{[l]}$, with an additional dimension for the batch size ($m$).\n",
    "\n",
    "## Ordering of Variables\n",
    "- While the convention used here lists height and width before the number of channels ($nH \\times nW \\times nC$), note that some sources use a different ordering, with channels listed first ($nC \\times nH \\times nW$). Both conventions are valid, but consistency is key.\n",
    "\n",
    "Remember that you don't need to memorize all of these notations at once, as you'll become more familiar with them through practical exercises. The primary takeaway is to understand how one layer of a CNN performs computations to map activations from the previous layer to activations in the current layer. This knowledge will be crucial as we build deeper CNNs by stacking multiple layers together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b0fcb",
   "metadata": {},
   "source": [
    "### Example simple ConvNet:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/simple_conv.png\" alt=\"large img\" width=100% height=100%>\n",
    "\n",
    "In this example, we discussed the architecture of a deep convolutional neural network (ConvNet) for image classification. The input image was 39x39x3 in size, where 39 represents both height and width, and 3 denotes the number of channels (e.g., RGB). The ConvNet comprised several layers, and we covered some key details:\n",
    "\n",
    "1. **Layer 1 (Convolutional Layer):**\n",
    "   - Filter size (f1): 3x3\n",
    "   - Stride (s1): 1\n",
    "   - Padding (p1): None (Valid Convolution)\n",
    "   - Number of filters: 10\n",
    "   - Output dimensions: 37x37x10\n",
    "\n",
    "2. **Layer 2 (Convolutional Layer):**\n",
    "   - Filter size (f2): 5x5\n",
    "   - Stride (s2): 2\n",
    "   - Padding (p2): None (Valid Convolution)\n",
    "   - Number of filters: 20\n",
    "   - Output dimensions: 17x17x20\n",
    "\n",
    "3. **Layer 3 (Convolutional Layer):**\n",
    "   - Filter size (f3): 5x5\n",
    "   - Stride (s3): 2\n",
    "   - Padding (p3): None (Valid Convolution)\n",
    "   - Number of filters: 40\n",
    "   - Output dimensions: 7x7x40\n",
    "\n",
    "4. **Flatten Layer:**\n",
    "   - The output of the last convolutional layer (7x7x40) is flattened into a vector of 1,960 units.\n",
    "\n",
    "5. **Final Layer (Logistic Regression or Softmax):**\n",
    "   - The flattened vector is fed into a logistic regression or softmax unit for the final prediction.\n",
    "\n",
    "This example illustrates the typical progression in ConvNets, where the height and width of the activations tend to decrease while the number of channels generally increases as you go deeper into the network. The specific hyperparameters like filter sizes, strides, padding, and the number of filters play a crucial role in the network's design.\n",
    "\n",
    "In ConvNets, there are three common types of layers:\n",
    "- Convolutional layers (Conv)\n",
    "- Pooling layers (Pool)\n",
    "- Fully connected layers (FC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7a047",
   "metadata": {},
   "source": [
    "# Pooling layer: Max pooling\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/max_pool.png\" alt=\"large img\" width=100% height=100%>\n",
    "\n",
    "# Understanding Max Pooling in ConvNets\n",
    "\n",
    "In Convolutional Neural Networks (ConvNets), pooling layers are often employed alongside convolutional layers to reduce representation size, accelerate computations, and enhance feature robustness. Let's explore an example of max pooling and its purpose.\n",
    "\n",
    "Consider a 4x4 input grid and the application of max pooling to yield a 2x2 output. The process is straightforward: divide the 4x4 input into regions and assign colors to each quadrant. For the 2x2 output, each value is determined by selecting the maximum number within the corresponding shaded region. Consequently, this employs a filter size of 2x2 because 2x2 regions are considered, and the stride is set to 2, progressing from one region to another.\n",
    "\n",
    "Max pooling essentially preserves features that exist anywhere within the filter. If a specific feature is detected within one of these quadrants, it remains intact in the max-pooled output. Conversely, if a feature isn't detected, the maximum operation keeps the value relatively small. This way, max pooling retains significant features while reducing irrelevant information.\n",
    "\n",
    "It's important to note that, although the intuition behind max pooling suggests it preserves critical features, its effectiveness has primarily been observed through experimentation. The exact reason for its success remains a subject of debate.\n",
    "\n",
    "An intriguing characteristic of max pooling is that it relies on a set of hyperparameters but introduces no learnable parameters. Once you define the filter size (f) and stride (s), the operation becomes a fixed computation, making it unsuitable for gradient descent adjustments.\n",
    "\n",
    "In summary, max pooling is a common pooling technique in ConvNets, serving to downsample representations while retaining essential features. Its effectiveness is well-established, even though the precise reasoning behind it is still a subject of inquiry.\n",
    "\n",
    "#### Another example\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/max_pool_in.png\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "Suppose you have a $5\\times5$ input and apply max pooling with a $3\\times3$ filter and a stride of 1, yielding a $3\\times3$ output. The output elements are computed based on the max value within each filter's region. For example, in the upper-left corner of the input, you take the max of the $3\\times3$ region to get 9. The same process continues for the entire input, resulting in the $3\\times3$ output. This formula works similarly to the ones for conv layers, using $ (n + 2p - f / s) + 1 $ to compute the output size.\n",
    "\n",
    "When dealing with a 3D input (e.g., $5\\times5\\times2$), max pooling retains the same dimensions in the output ($3\\times3\\times2$). Each channel is processed independently through the max pooling operation.\n",
    "\n",
    "Max pooling is a widely used technique due to its simplicity and effectiveness. Notably, it has no learnable parameters; hyperparameters like filter size ($f$) and stride ($s$) are set manually. Usually, max pooling does not involve padding ($p = 0$) unless in specific cases.\n",
    "\n",
    "## Average Pooling\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/avg_pooling.png\" alt=\"large img\" width=80% height=80%>\n",
    "\n",
    "Average pooling, another type of pooling, computes the average of values within a specified region rather than selecting the maximum. Although it is less common than max pooling, it has its uses, especially for collapsing large representations in deep neural networks.\n",
    "\n",
    "In summary, pooling layers in ConvNets play a crucial role in reducing dimensionality and enhancing feature extraction. Max pooling, with its simplicity and effectiveness, is the preferred choice in most cases. The key hyperparameters to set are filter size ($f$) and stride ($s$), with padding ($p$) rarely applied. Importantly, pooling layers do not introduce learnable parameters and are fixed computations within the network.\n",
    "\n",
    "\n",
    "\n",
    "# Summary \n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/sum_pool.png\" alt=\"large img\" width=80% height=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848045fb",
   "metadata": {},
   "source": [
    "### Code example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ccba0b",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ace08d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Layer Output Shape after ReLU: torch.Size([1, 10, 4, 4])\n",
      "Max Pooling Layer Output Shape: torch.Size([1, 10, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Import the functional module for activations\n",
    "\n",
    "# Demo input data: Batch size 1, 3 channels (RGB), 6x6 image\n",
    "demo_input = torch.randn(1, 3, 6, 6)\n",
    "\n",
    "# Define the Convolutional Layer\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, padding=0)\n",
    "\n",
    "# Pass the input through the convolutional layer\n",
    "output = conv_layer(demo_input)\n",
    "\n",
    "# Apply ReLU activation function to the output\n",
    "output = F.relu(output)\n",
    "\n",
    "print(\"Convolutional Layer Output Shape after ReLU:\", output.shape)\n",
    "\n",
    "# Define the Max Pooling Layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Pass the output of the convolutional layer through the max pooling layer\n",
    "output_max_pool = max_pool_layer(output)\n",
    "\n",
    "print(\"Max Pooling Layer Output Shape:\", output_max_pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd7d80",
   "metadata": {},
   "source": [
    "### Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ede86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Pooling Layer Output Shape: torch.Size([1, 10, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define the Average Pooling Layer\n",
    "avg_pool_layer = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Pass the output of the convolutional layer through the average pooling layer\n",
    "output_avg_pool = avg_pool_layer(output)\n",
    "\n",
    "print(\"Average Pooling Layer Output Shape:\", output_avg_pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a20ed",
   "metadata": {},
   "source": [
    "## CNN Example:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/convolution_with_filters.png\" alt=\"python CNN\" width=76% height=58% title=\"CNN Convolution\"> \n",
    "\n",
    "## Total Param:\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/total_param.png\" alt=\"python CNN\" width=76% height=58% title=\"CNN Convolution\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7ae44",
   "metadata": {},
   "source": [
    "### Why Convolutions?\n",
    "\n",
    "In convolutional neural networks (ConvNets), two key principles contribute to their effectiveness: parameter sharing and sparse connections. \n",
    "\n",
    "**Parameter sharing** is based on the idea that a feature detector, like one that detects vertical edges, can be reused at different positions in an image. This sharing of parameters significantly reduces the number of parameters, making the network more efficient. Whether it's detecting low-level features like edges or higher-level features like facial features, sharing parameters across the image helps in feature extraction.\n",
    "\n",
    "**Sparse connections** are another factor that reduces the number of parameters. When computing an output, each unit depends only on a small local region of the input. For example, a 3x3 convolutional operation considers only a 3x3 grid of input pixels, resulting in sparse connections. This property helps the network focus on relevant information and prevents overfitting.\n",
    "\n",
    "ConvNets also excel at capturing **translation invariance**, meaning that a shifted image (e.g., a cat image moved a few pixels to the right) should still be recognized as the same object. Convolutional structures inherently encode this property, making them robust to small translations.\n",
    "\n",
    "In training a ConvNet, you start with randomly initialized parameters (weights and biases) and compute a cost function (e.g., mean squared error or cross-entropy) based on the network's predictions and the true labels. You then use optimization algorithms like gradient descent, momentum, RMSProp, or Adam to adjust the parameters and minimize the cost function. This process allows you to build effective detectors for various tasks, such as cat detection.\n",
    "\n",
    "Overall, ConvNets leverage parameter sharing, sparse connections, and translation invariance to efficiently process images and extract meaningful features, making them a powerful tool for computer vision tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f717c",
   "metadata": {},
   "source": [
    "### Training a Neural Network\n",
    "\n",
    "<img src=\"Lecture_Image_Files/lec_16/Images/training_neural.png\" alt=\"large img\" width=100% height=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdda770",
   "metadata": {},
   "source": [
    "### Softmax Function Explanation\n",
    "\n",
    "The Softmax function is a mathematical function used in machine learning and deep learning for converting a vector of real numbers into a probability distribution. It is commonly used as the activation function in the output layer of classification models.\n",
    "\n",
    "Given an input vector **z = [z1, z2, ..., zn]**, the Softmax function calculates the probability **pi** for each element **zi** as:\n",
    "\n",
    "![Softmax Equation](https://latex.codecogs.com/svg.latex?p_i%20%3D%20%5Cfrac%7Be%5E%7Bz_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%20e%5E%7Bz_j%7D%7D)\n",
    "\n",
    "where **e^x** denotes the exponential function.\n",
    "\n",
    "The Softmax function exponentiates each element of the input vector **z** and then normalizes the results to ensure that the sum of the probabilities is equal to 1, producing a vector of probabilities representing the likelihood of each class.\n",
    "\n",
    "In PyTorch, you can use the `torch.nn.functional.softmax` function to apply softmax to a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2c10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example input tensor (logits)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = F.softmax(logits, dim=0)\n",
    "\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cd9d9",
   "metadata": {},
   "source": [
    "## Now let's make a CNN and see a real example how it works. For this we will be using MNIST dataset. \n",
    "\n",
    "### Project Process:\n",
    "1. Prepare the data\n",
    "2. Build the model\n",
    "3. Train the model\n",
    "4. Analyze the model's result\n",
    "\n",
    "#### Brief about MNIST dataset\n",
    "\n",
    "The MNIST dataset is a widely used benchmark dataset in the field of machine learning and computer vision. It stands for the Modified National Institute of Standards and Technology (MNIST), as it is derived from a larger dataset collected by the US National Institute of Standards and Technology.\n",
    "\n",
    "The MNIST dataset consists of a collection of grayscale images of handwritten digits from 0 to 9. Each image is a 28x28-pixel square, representing a digit written in a consistent format. The dataset is divided into two main parts: a training set and a test set.\n",
    "\n",
    "The training set contains 60,000 images, while the test set contains 10,000 images. This split allows researchers and practitioners to evaluate the performance of machine learning algorithms on unseen data. The MNIST dataset has become a standard benchmark for evaluating and comparing the performance of various image classification algorithms, especially in the context of deep learning.\n",
    "\n",
    "The goal of the MNIST dataset is to accurately classify the handwritten digits into their corresponding classes (0 to 9) based on the pixel values of the images. It has been widely used as an introductory dataset for learning image classification techniques and developing and testing various machine learning models.\n",
    "\n",
    "Due to its simplicity and availability, the MNIST dataset has been extensively studied and has led to significant advancements in the field of machine learning and deep learning. It serves as a starting point for beginners to understand and implement image classification algorithms and has become a reference dataset for comparing the performance of different models on more complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecf013",
   "metadata": {},
   "source": [
    "#### Code for CNN model and training the model using MNIST dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b51551",
   "metadata": {},
   "source": [
    "Before we begin let's see how transform module work in pytorch. Because we will be using this module in order to pre-process our image. \n",
    "\n",
    "In PyTorch, the transform module provides a set of common image transformations that can be applied to datasets. These transformations allow you to preprocess and augment the data before feeding it into the neural network. The transform module is part of the torchvision library, which is a PyTorch package specifically designed for working with image datasets.\n",
    "\n",
    "The transform module provides various operations such as resizing, cropping, normalization, and data augmentation techniques like random rotations, flips, and color jittering. These transformations can be applied to both the training and test datasets to ensure consistency and appropriate preprocessing.\n",
    "\n",
    "Here's a sample code snippet that demonstrates the usage of the transform module in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbdc99",
   "metadata": {},
   "source": [
    "## Note: separate each code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bfd55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 9912422/9912422 [00:01<00:00, 6883035.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 28881/28881 [00:00<00:00, 82181610.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:00<00:00, 5087638.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 4542/4542 [00:00<00:00, 12863287.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset without any transformations\n",
    "dataset = MNIST(root='data/', train=True, download=True)\n",
    "\n",
    "# Select an image from the dataset\n",
    "image, _ = dataset[0]\n",
    "\n",
    "# Define the transformations to apply to the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),           # Resize the image to (32, 32)\n",
    "    transforms.RandomHorizontalFlip(),     # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(99),  \n",
    "    transforms.ToTensor(),                  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))    # Normalize the image with mean and standard deviation\n",
    "])\n",
    "\n",
    "# Apply the transformations to the image\n",
    "transformed_image = transform(image)\n",
    "\n",
    "# Display a comparison of the original and transformed images\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].imshow(image, cmap='gray')\n",
    "axs[0].set_title(\"Original Image\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(transformed_image.squeeze().numpy(), cmap='gray')\n",
    "axs[1].set_title(\"Transformed Image\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd230a3",
   "metadata": {},
   "source": [
    "##### as we can see the image has been rotated by a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda0a9b",
   "metadata": {},
   "source": [
    "#### Now let's jump right into the project:\n",
    "\n",
    "In this project, we will train our CNN model on MNIST dataset that can predict handwritten digits from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52a45754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6520bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 10146951.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 28881/28881 [00:00<00:00, 18939289.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:00<00:00, 5514436.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 4542/4542 [00:00<00:00, 15425529.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295844\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.249347\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.222004\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.151714\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.084718\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.978025\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.832004\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 1.682547\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.495733\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 1.267230\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.000248\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.774379\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.849698\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.644116\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.598154\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.471154\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.628364\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.580138\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.462176\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.519410\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.463268\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.475191\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.386370\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.315407\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.507737\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.604779\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.202801\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.476287\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.611373\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.220134\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.371925\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.317134\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.192658\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.164823\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.327417\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.192132\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.515406\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.474224\n",
      "\n",
      "Test set: Average loss: 0.3253, Accuracy: 9085/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.480623\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.545669\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.228401\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.632017\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.238872\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.356368\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.131020\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.226347\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.271768\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.496061\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.300913\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.408475\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.157522\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.313831\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.391600\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.169287\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.149629\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.118067\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.232683\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.211425\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.261559\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.249143\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.415499\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.131795\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.214207\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.668988\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.375513\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.205240\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.176857\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.252206\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.298532\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.361041\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.409594\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.253552\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.351783\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.158560\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.193942\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.537093\n",
      "\n",
      "Test set: Average loss: 0.2615, Accuracy: 9252/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.365461\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.347471\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.340890\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.175294\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.437823\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.128981\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.530612\n",
      "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.136351\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.280645\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.218347\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.302520\n",
      "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.435687\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.398277\n",
      "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.210195\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.305443\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.140388\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.261933\n",
      "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.152349\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.263207\n",
      "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.097110\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.177426\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.160016\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.201287\n",
      "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.362414\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.286542\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.293158\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.165163\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.253256\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.164669\n",
      "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.408449\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.099353\n",
      "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.308135\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.254907\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.383595\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.287743\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.212889\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.109731\n",
      "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.498654\n",
      "\n",
      "Test set: Average loss: 0.2175, Accuracy: 9349/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.070788\n",
      "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.226662\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.163690\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.368997\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.107484\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.160466\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.099886\n",
      "Train Epoch: 4 [11200/60000 (19%)]\tLoss: 0.128986\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.416390\n",
      "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.074819\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.344167\n",
      "Train Epoch: 4 [17600/60000 (29%)]\tLoss: 0.116998\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.086362\n",
      "Train Epoch: 4 [20800/60000 (35%)]\tLoss: 0.098647\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.128599\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.375059\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.075754\n",
      "Train Epoch: 4 [27200/60000 (45%)]\tLoss: 0.402769\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.055159\n",
      "Train Epoch: 4 [30400/60000 (51%)]\tLoss: 0.097950\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.109939\n",
      "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.284605\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.134385\n",
      "Train Epoch: 4 [36800/60000 (61%)]\tLoss: 0.340201\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.144110\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.357696\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.139699\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.030035\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.136471\n",
      "Train Epoch: 4 [46400/60000 (77%)]\tLoss: 0.169885\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.265010\n",
      "Train Epoch: 4 [49600/60000 (83%)]\tLoss: 0.179311\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.060702\n",
      "Train Epoch: 4 [52800/60000 (88%)]\tLoss: 0.209677\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.069148\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.287883\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.077833\n",
      "Train Epoch: 4 [59200/60000 (99%)]\tLoss: 0.246391\n",
      "\n",
      "Test set: Average loss: 0.1867, Accuracy: 9458/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.065847\n",
      "Train Epoch: 5 [1600/60000 (3%)]\tLoss: 0.180458\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.158751\n",
      "Train Epoch: 5 [4800/60000 (8%)]\tLoss: 0.405047\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.195659\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.257440\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.355920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [11200/60000 (19%)]\tLoss: 0.071013\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.397074\n",
      "Train Epoch: 5 [14400/60000 (24%)]\tLoss: 0.094892\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.105214\n",
      "Train Epoch: 5 [17600/60000 (29%)]\tLoss: 0.185680\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.371011\n",
      "Train Epoch: 5 [20800/60000 (35%)]\tLoss: 0.185564\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.136829\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.093076\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.089156\n",
      "Train Epoch: 5 [27200/60000 (45%)]\tLoss: 0.236428\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.110313\n",
      "Train Epoch: 5 [30400/60000 (51%)]\tLoss: 0.056911\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.170126\n",
      "Train Epoch: 5 [33600/60000 (56%)]\tLoss: 0.096803\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.123637\n",
      "Train Epoch: 5 [36800/60000 (61%)]\tLoss: 0.157834\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.338008\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.146575\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.166716\n",
      "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.073585\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.143369\n",
      "Train Epoch: 5 [46400/60000 (77%)]\tLoss: 0.252475\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.089642\n",
      "Train Epoch: 5 [49600/60000 (83%)]\tLoss: 0.138645\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.135058\n",
      "Train Epoch: 5 [52800/60000 (88%)]\tLoss: 0.179792\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.443392\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.156069\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.176011\n",
      "Train Epoch: 5 [59200/60000 (99%)]\tLoss: 0.082773\n",
      "\n",
      "Test set: Average loss: 0.1694, Accuracy: 9502/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(in_features = 64 * 14 * 14 , out_features = 128)\n",
    "        self.fc2 = nn.Linear(in_features = 128, out_features = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "#define training function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "# Define test funcion\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "# Driver class\n",
    "\n",
    "batch_size = 32 \n",
    "use_cuda = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_epoch = 5\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': batch_size}\n",
    "\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=batch_size, \n",
    "                      shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, \n",
    "                      shuffle=False)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "    if num_epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "805abd67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc5804f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([32, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82671f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3, 8, 9, 0, 5, 8, 3, 0, 1, 7, 9, 1, 2, 2, 1, 2, 0, 4, 9, 3, 8, 7, 0,\n",
      "        8, 0, 0, 0, 1, 6, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "#check labels\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146e71af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Test the model on a single input\n",
    "model.eval()\n",
    "# Load and preprocess the image\n",
    "image_path = '/home/mustafa/Downloads/hand.jpg'  # Replace with the path to your grayscale image\n",
    "image = Image.open(image_path)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to 28x28\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure it's grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize the image\n",
    "])\n",
    "image = transform(image).to(device)\n",
    "image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "\n",
    "# Interpret the model's output (e.g., classification or regression)\n",
    "# Replace this part with your specific task (e.g., softmax for classification)\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Define a function to display the image and label\n",
    "def show_image_with_label(image, label):\n",
    "    image = image.squeeze(0).squeeze(0)  # Remove the batch dimension\n",
    "    plt.imshow(image.cpu().numpy(), cmap='gray')\n",
    "    plt.title(f\"Predicted Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the image with the predicted label\n",
    "show_image_with_label(image, predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85c366",
   "metadata": {},
   "source": [
    "## Calculating Total Number of Parameters in a CNN Architecture\n",
    "\n",
    "To calculate the total number of parameters in a Convolutional Neural Network (CNN) architecture in PyTorch, you can iterate through all the trainable parameters of the network and count the number of elements in each parameter. The total number of parameters will be the sum of all these counts.\n",
    "\n",
    "Here's the code to calculate the total number of parameters in a given CNN architecture:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b512da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.weight, Parameters: 432\n",
      "Layer: conv1.bias, Parameters: 16\n",
      "Layer: conv2.weight, Parameters: 4608\n",
      "Layer: conv2.bias, Parameters: 32\n",
      "Layer: fc1.weight, Parameters: 262144\n",
      "Layer: fc1.bias, Parameters: 128\n",
      "Layer: fc2.weight, Parameters: 1280\n",
      "Layer: fc2.bias, Parameters: 10\n",
      "Total Number of Parameters: 268650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define your CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the CNN\n",
    "net = SimpleCNN()\n",
    "\n",
    "# Calculate the total number of parameters layer by layer\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Parameters: {param.numel()}\")\n",
    "\n",
    "print(\"Total Number of Parameters:\", sum(p.numel() for p in net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90107320",
   "metadata": {},
   "source": [
    "#### we use net.named_parameters() to get an iterator over all the parameters in the network along with their corresponding names. We then iterate through these parameters and print the name of the layer along with the number of parameters in that layer. Finally, we print the total number of trainable parameters in the entire network.\n",
    "\n",
    "#### When you run this code, it will display the parameter counts for each layer and the total number of parameters in the CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4bc44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
